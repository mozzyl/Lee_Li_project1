---
title: "Project 1"
subtitle: "CMDA 4654"
author: "Charles Lee, Andrew Li"
date: "11/10/20"
output:
  pdf_document:
    highlight: haddock
keep_tex: no
number_sections: no
html_document:
  df_print: paged
geometry: margin = 0.5in
header-includes:
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage{xcolor}
editor_options:
  chunk_output_type: console
documentclass: article
urlcolor: blue
---
<!-- The above is set to automatically compile to a .pdf file.   -->


```{r setup, include=FALSE}
# This is the setup chunk
#  Here you can set global options for the entire document

library(knitr) # I recommend doing this here

# Although you can call functions from a library using the following notation
#  without loading the entire library.
knitr::opts_chunk$set(echo = TRUE, 
                      comment = NA, # Required
                      fig.path = "./figures/",  # Store all figures here in relative path (make the folder first)
                      fig.align = "center",
                      fig.width = 5, # change the size of figures
                      fig.height = 5,
                      message = FALSE, # Turn off load messages
                      warning = FALSE # Turn off warnings
                      )
```

\clearpage

```{r include=FALSE}

# You should set your working directory at the very beginning of your R Markdown file

setwd("C:/CMDA 4654/Project1")

```


<!-- ---------------------------------------------------------------------------------------------------- -->
<!-- ---------------- Project 1 Problems start below these lines ----------------------------------------- -->
<!-- ---------------------------------------------------------------------------------------------------- -->

# Part 1 - LOESS/LOWESS Regression

```{r}
library(ggplot2)
load("ozone.RData")
#data("ozone")
ggplot(ozone, aes(x = temperature, y = ozone)) + theme_bw() + geom_point()
```

## Problem 1
First consider the ozone dataset, see Canvas for the ozone.RData file.

1. Fit polynomials of different degrees between 1 and 6 with ozone being regressed upon temperature. Which polynomial fit appears to work the best?
```{r}

```

2. Use your function to determine LOESS regression fits on the data. Use span = 0.25 to 0.75 with steps of 0.05. Do this for both degree = 1 and degree = 2. List all of the results in a table. Plot the three “best” degree = 1 and three “best” degree = 2 fits that you determined, make sure to put the appropriate span in the title for each plot (there should be a total of 6 plots). You determined the “best” by comparing the residual standard errors. However if you visually inspect the best compared to the 2nd and 3rd best fits, do you feel that you may have over-fit the data?
```{r}

```

3. Be sure to compare your results with that found from the built-in loess() function and your plots with that which is obtained from ggplot() + geom_smooth()/stat_smooth().
```{r}
# Degree 1
L1 <- loess(ozone ~ temperature, data=ozone, span = 0.25, degree = 1)
L2 <- loess(ozone ~ temperature, data=ozone, span = 0.30, degree = 1)
L3 <- loess(ozone ~ temperature, data=ozone, span = 0.35, degree = 1)
L4 <- loess(ozone ~ temperature, data=ozone, span = 0.40, degree = 1)
L5 <- loess(ozone ~ temperature, data=ozone, span = 0.45, degree = 1)
L6 <- loess(ozone ~ temperature, data=ozone, span = 0.50, degree = 1)
L7 <- loess(ozone ~ temperature, data=ozone, span = 0.55, degree = 1)
L8 <- loess(ozone ~ temperature, data=ozone, span = 0.60, degree = 1)
L9 <- loess(ozone ~ temperature, data=ozone, span = 0.65, degree = 1)
L10 <- loess(ozone ~ temperature, data=ozone, span = 0.70, degree = 1)
L11 <- loess(ozone ~ temperature, data=ozone, span = 0.75, degree = 1)

# Table of Residual Standard Errors when degree=1
d1 <- matrix(c(L1$s,L2$s,L3$s,L4$s,L5$s,L6$s,L7$s,L8$s,L9$s,L10$s,L11$s), ncol = 11, byrow=TRUE)
colnames(d1) <- c("Span=0.25","Span=0.30","Span=0.35","Span=0.40","Span=0.45","Span=0.50","Span=0.55","Span=0.60","Span=0.65","Span=0.70","Span=0.75")
rownames(d1) <- c("Residual Standard Error")
d1

# Predict Loess of 3 "best" ones
# get smooted output
smoothed65 <- predict(L9)
smoothed70 <- predict(L10) 
smoothed75 <- predict(L11) 

# Plot
plot(ozone$ozone, x=ozone$temperature, type="l", main="Loess Smoothing and Prediction, degree = 1", xlab="Temperature", ylab="Ozone (Median)")
lines(smoothed65, x=ozone$temperature, col="red")
lines(smoothed70, x=ozone$temperature, col="green")
lines(smoothed75, x=ozone$temperature, col="blue")
```
The graph is more accurate when the Residual Standard Error is lower. Looking at the table d1, we see that the Residual Standard Error decreases as span increases which makes sense since increasing the span is the smoothing parameter, and the larger the smoothing parameter is, the more accurate the graph becomes. Therefore the last 3 loess variables (L9,L10,L11) with the largest spans are the "best".


TEST
```{r}
L1
L1$s
L1$residuals
L1

sigma(lm(ozone ~ temperature, data=ozone))
```




```{r}
# Degree 2
L11 <- loess(ozone ~ temperature, data=ozone, span = 0.25, degree = 2)
L22 <- loess(ozone ~ temperature, data=ozone, span = 0.30, degree = 2)
L33 <- loess(ozone ~ temperature, data=ozone, span = 0.35, degree = 2)
L44 <- loess(ozone ~ temperature, data=ozone, span = 0.40, degree = 2)
L55 <- loess(ozone ~ temperature, data=ozone, span = 0.45, degree = 2)
L66 <- loess(ozone ~ temperature, data=ozone, span = 0.50, degree = 2)
L77 <- loess(ozone ~ temperature, data=ozone, span = 0.55, degree = 2)
L88 <- loess(ozone ~ temperature, data=ozone, span = 0.60, degree = 2)
L99 <- loess(ozone ~ temperature, data=ozone, span = 0.65, degree = 2)
L1010 <- loess(ozone ~ temperature, data=ozone, span = 0.70, degree = 2)
L1111 <- loess(ozone ~ temperature, data=ozone, span = 0.75, degree = 2)

# Table of Residual Standard Errors when degree=2
d2 <- matrix(c(L11$s,L22$s,L33$s,L44$s,L55$s,L66$s,L77$s,L88$s,L99$s,L1010$s,L1111$s), ncol = 11, byrow=TRUE)
colnames(d2) <- c("Span=0.25","Span=0.30","Span=0.35","Span=0.40","Span=0.45","Span=0.50","Span=0.55","Span=0.60","Span=0.65","Span=0.70","Span=0.75")
rownames(d2) <- c("Residual Standard Error")
d2

# Predict Loess of 3 "best" ones
# get smooted output
smoothed25 <- predict(L11) 
smoothed30 <- predict(L22) 
smoothed35 <- predict(L33) 

# Plot
plot(ozone$ozone, x=ozone$temperature, type="l", main="Loess Smoothing and Prediction, degree = 2", xlab="Temperature", ylab="Ozone (Median)")
lines(smoothed25, x=ozone$temperature, col="red")
lines(smoothed30, x=ozone$temperature, col="green")
lines(smoothed35, x=ozone$temperature, col="blue")
```
Here, there the three "best" are the first 3 (L11,L22,L33) which is the oposite from degree=1. 


## Problem 2 
Consider the mcycle dataset from the MASS library package.
```{r}
library(MASS)
data("mcycle")
ggplot(mcycle, aes(x = times, y = accel)) + theme_bw() + geom_point()
```

1. This dataset is notoriously difficult to fit with polynomial regression. Later we will study how to use regression trees to fit this data. Until then, determine the three “best” degree = 1 and three “best” degree = 2 LOESS regression fits by finding the best span between 0.25 and 0.75 with steps of 0.05 for each degree. Report your answers in a table. Plot the three best first for both degree = 1 and degree = 2. Based upon a visual inspection, which models provide the “best” fit?
```{r}

```

2. Be sure to compare your results with that found from the built-in loess() function and your plots with that which is obtained from ggplot() + geom_smooth()/stat_smooth().
```{r}
# Degree 1
Lo1 <- loess(accel ~ times, data=mcycle, span = 0.25, degree = 1)
Lo2 <- loess(accel ~ times, data=mcycle, span = 0.30, degree = 1)
Lo3 <- loess(accel ~ times, data=mcycle, span = 0.35, degree = 1)
Lo4 <- loess(accel ~ times, data=mcycle, span = 0.40, degree = 1)
Lo5 <- loess(accel ~ times, data=mcycle, span = 0.45, degree = 1)
Lo6 <- loess(accel ~ times, data=mcycle, span = 0.50, degree = 1)
Lo7 <- loess(accel ~ times, data=mcycle, span = 0.55, degree = 1)
Lo8 <- loess(accel ~ times, data=mcycle, span = 0.60, degree = 1)
Lo9 <- loess(accel ~ times, data=mcycle, span = 0.65, degree = 1)
Lo10 <- loess(accel ~ times, data=mcycle, span = 0.70, degree = 1)
Lo11 <- loess(accel ~ times, data=mcycle, span = 0.75, degree = 1)

# Table of Residual Standard Errors when degree=1
de1 <- matrix(c(Lo1$s,Lo2$s,Lo3$s,Lo4$s,Lo5$s,Lo6$s,Lo7$s,Lo8$s,Lo9$s,Lo10$s,Lo11$s), ncol = 11, byrow=TRUE)
colnames(d1) <- c("Span=0.25","Span=0.30","Span=0.35","Span=0.40","Span=0.45","Span=0.50","Span=0.55","Span=0.60","Span=0.65","Span=0.70","Span=0.75")
rownames(d1) <- c("Residual Standard Error")
de1

# Predict Loess of 3 "best" ones
# get smooted output
smoothedd25 <- predict(Lo1) 
smoothedd30 <- predict(Lo2) 
smoothedd35 <- predict(Lo3) 

# Plot
plot(mcycle$accel, x=mcycle$times, type="l", main="Loess Smoothing and Prediction, degree = 1", xlab="Times", ylab="Accel (Median)")

plot(mcycle$accel ~ mcycle$times, type="l", main="Loess Smoothing and Prediction, degree = 1", xlab="Times", ylab="Accel (Median)")

lines(smoothedd25, x=mcycle$times, col="red")
lines(smoothedd30, x=mcycle$times, col="green")
lines(smoothedd35, x=mcycle$times, col="blue")
```
The graph is more accurate when the Residual Standard Error is lower. Looking at the table de1, we see that the Residual Standard Error increases as span increases. Therefore the first 3 loess variables (Lo1,Lo2,Lo3) with the largest spans are the "best".

```{r}
# Degree 2
Lo11 <- loess(accel ~ times, data=mcycle, span = 0.25, degree = 2)
Lo22 <- loess(accel ~ times, data=mcycle, span = 0.30, degree = 2)
Lo33 <- loess(accel ~ times, data=mcycle, span = 0.35, degree = 2)
Lo44 <- loess(accel ~ times, data=mcycle, span = 0.40, degree = 2)
Lo55 <- loess(accel ~ times, data=mcycle, span = 0.45, degree = 2)
Lo66 <- loess(accel ~ times, data=mcycle, span = 0.50, degree = 2)
Lo77 <- loess(accel ~ times, data=mcycle, span = 0.55, degree = 2)
Lo88 <- loess(accel ~ times, data=mcycle, span = 0.60, degree = 2)
Lo99 <- loess(accel ~ times, data=mcycle, span = 0.65, degree = 2)
Lo1010 <- loess(accel ~ times, data=mcycle, span = 0.70, degree = 2)
Lo1111 <- loess(accel ~ times, data=mcycle, span = 0.75, degree = 2)

# Table of Residual Standard Errors when degree=1
de2 <- matrix(c(Lo11$s,Lo22$s,Lo33$s,Lo44$s,Lo55$s,Lo66$s,Lo77$s,Lo88$s,Lo99$s,Lo1010$s,Lo1111$s), ncol = 11, byrow=TRUE)
colnames(d1) <- c("Span=0.25","Span=0.30","Span=0.35","Span=0.40","Span=0.45","Span=0.50","Span=0.55","Span=0.60","Span=0.65","Span=0.70","Span=0.75")
rownames(d1) <- c("Residual Standard Error")
de2

# sort(de2)

# Predict Loess of 3 "best" ones
# get smooted output
smootheddd30 <- predict(Lo22) 
smootheddd35 <- predict(Lo33) 
smootheddd25 <- predict(Lo11) 

# Plot
plot(mcycle$accel, x=mcycle$times, type="l", main="Loess Smoothing and Prediction, degree = 2", xlab="Times", ylab="Accel (Median)")
lines(smootheddd30, x=mcycle$times, col="red")
lines(smootheddd35, x=mcycle$times, col="green")
lines(smootheddd25, x=mcycle$times, col="blue")
```
Looking at the table de2, we see that the Residual Standard Errors. According to de2, the 3 "best" loess variables are Lo22,Lo33,Lo11. Degree 1 and Degree 2 are very similar.




# Part 2 k-Nearest Neighbors for Classification

Using your Function

Use your function on the Auto dataset from the ISLR library. You can read about the variables using help("Auto")
```{r}
# Some pre-processing
#install.packages('ISLR')
library(ISLR)
# Remove the name of the car model and change the origin to categorical with actual name
Auto_new <- Auto[, -9]
# Lookup table
newOrigin <- c("USA", "European", "Japanese")
Auto_new$origin <- factor(newOrigin[Auto_new$origin], newOrigin)
# Look at the first 6 observations to see the final version
head(Auto_new)
```

1. Randomly split your data into two data frames. Use 70% of your data for the training data and 30% for the testing data.
```{r}
## Generate a random number that is 70% for the training data
ran70 <- sample(1:nrow(Auto_new), 0.7 * nrow(Auto_new)) 

## Generate a random number that is 30% for the testing data
ran30 <- sample(1:nrow(Auto_new), 0.3 * nrow(Auto_new)) 

## the normalization function is created
nor <-function(x) { 
  (x -min(x))/(max(x)-min(x))
  }

## Run nomalization on first 7 coulumns of dataset because they are the predictors
Auto_norm <- as.data.frame(lapply(Auto_new[,c(1,2,3,4,5,6)], nor))

summary(Auto_norm)

## extract training set
Auto_train <- Auto_norm[ran70,]

## extract testing set
Auto_test <- Auto_norm[ran30,]

##extract 5th column of train dataset because it will be used as 'cl' argument in knn function.
Auto_target_category <- Auto_new[ran70,5]

##extract 5th column if test dataset to measure the accuracy
Auto_test_category <- Auto_new[ran30,5]

## load the package class
library(class)
## run knn function
pr <- knn(Auto_train, Auto_test, cl=Auto_target_category, k=13)
 
## create confusion matrix
tab <- table(pr, Auto_test_category)

## this function divides the correct predictions by total number of predictions that tell us how accurate the model is.
accuracy <- function(x){
  sum(diag(x)/(sum(rowSums(x)))) * 100
}

## accuracy of Confusion Matrix
accuracy(tab)

```

2. Use your mykNN() function with your training and testing data for several values of k and record your accuracy.
```{r}

```

3. Make a table of the accuracy for different values of k. Display this table nicely using a table maker like kable().
```{r}

```

4. Make a plot of the accuracy versus k to determine the best number of neighbors to use.
```{r}

```

5. Show the final confusion matrix for the best value of k and make sure the accuracy is stated.
```{r}

```


